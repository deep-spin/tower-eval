gen_data_dir: "TowerEval-Data-v0.1/data/instructions_data/5_shot"
eval_data_dir: "TowerEval-Data-v0.1/data/raw_data"
gen_output_dir: "TowerEval-Data-v0.1/generations/5_shot"
eval_output_dir: "TowerEval-Data-v0.1/evaluations/5_shot"
tasks: 
  - name: mt
    subtasks:
      flores.en-de:
      flores.en-fr:
      flores.en-pt:
      flores.en-nl:
      flores.en-es:
      flores.en-it:
      flores.en-zh:
        gen_args:
        eval_args:
          metrics:
            chrf:
            bleu:
              tokenizer: zh
            comet:
              batch_size: 16
      flores.en-ko:
        gen_args:
        eval_args:
          metrics:
            chrf:
            bleu:
              tokenizer: ko-mecab
            comet:
              batch_size: 16
      flores.en-ru:
      flores.de-en:
      flores.fr-en:
      flores.pt-en:
      flores.nl-en:
      flores.es-en:
      flores.it-en:
      flores.zh-en:
      flores.ko-en:
      flores.ru-en:
      wmt23.en-de:
      wmt23.en-ru:
      wmt23.en-zh:
        eval_args:
          metrics:
            chrf:
            bleu:
              tokenizer: zh
            comet:
              batch_size: 16
      wmt23.de-en:
      wmt23.ru-en:
      wmt23.zh-en:
      tico19.en-es:
      tico19.en-fr:
      tico19.en-pt:
      tico19.en-ru:
      tico19.en-zh:
        gen_args:
        eval_args:
          metrics:
            chrf:
            bleu:
              tokenizer: zh
            comet:
              batch_size: 16
    metrics:
      chrf:
      bleu:
      comet:
        batch_size: 16
models:
  - name: TowerBase-7B-v0.1
    type: vllm
    arguments:
      model_dir: Unbabel/TowerBase-7B-v0.1
      n_gpus: 1
      max_tokens: 1024
      run_async: True
      batch_size: -1
  - name: llama2-7b-hf
    type: vllm
    arguments:
      model_dir: meta-llama/Llama-2-7b-hf
      n_gpus: 1
      max_tokens: 1024
      run_async: True
      batch_size: -1
  - name: llama2-13b-hf
    type: vllm
    arguments:
      model_dir: meta-llama/Llama-2-13b-hf
      n_gpus: 1
      max_tokens: 1024
      run_async: True
      batch_size: -1
  - name: alma-pretrained-13b
    type: vllm
    arguments:
      model_dir: haoranxu/ALMA-13B
      n_gpus: 1
      max_tokens: 1024
      run_async: True
      batch_size: -1
  - name: llama2-70b-hf
    type: vllm
    arguments:
      model_dir: meta-llama/Llama-2-70b-hf
      n_gpus: 2
      max_tokens: 1024
      run_async: True
      batch_size: -1
  - name: mixtral-8x7B-v0.1
    type: vllm
    arguments:
      model_dir: mistralai/Mixtral-8x7B-v0.1
      n_gpus: 2
      max_tokens: 1024
      run_async: True
      batch_size: -1